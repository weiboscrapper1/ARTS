# 

# Algorithem

[ZigZag Conversion](https://github.com/weiboscrapper1/arts_leetcode/blob/master/src/main/java/practice/leetcode/algorithm/ZigZagConversion.java)

# Review

## [Out-Of-Memory Errors](https://www.dynatrace.com/resources/ebooks/javabook/other-java-memory-issues/)

内存不足错误指的是heap上没有足够的空间来创建新对象。在抛出内存不足错误前，JVM始终会触发垃圾回收来试着回收内存。大部分情况下，潜在的原因是配置内存不足或者是内存泄漏，但是还有一些别的原因。

下面是导致内存耗尽错误的重要原因：

- 配置内存不足
- 内存泄漏
- 内存碎片
- 额外的GC开销
- 为超大的临时对象分配空间

即使看起来还有足够的空余内存可用，内存碎片也会导致内存不足错误，原因是heap上没有一块连续的、足够大的空间来满足分配请求。大部分情况下，内存压缩能够保证这种情况不会发生，但是有些GC策略并不使用内存压缩。

一些JVM实现，比如Oracle HotSpot，在GC开销太大的情况下，仍然会抛出内存不足错误。该功能被设计用来方式接近常数垃圾回收——例如，耗费超过90%的执行时间在垃圾回收上，却只释放了不到2%的内存。配置更大的heap空间通常能够解决这个问题，但是如果不能，就需要使用heap dump来分析内存使用。

最后一个问题通过是开发人员考虑不周导致：程序逻辑尝试为超大的临时对象分配空间。既然JVM不能满足这样的请求，内存不足错误就被触发，事务也就被终止。由于没有heap dump，也没有空间分配分析工具能够指明这样的错误，这种问题很难定位。只能通过定位触发该错误的代码，然后解决问题。

### 流失率和高事务内存使用量（Churn Rate and High Transactional Memory Usage)

#### 问题

流失率(`Churn Rate`)用来描述每个事务或者时间片中分配的临时对象的个数。Java允许我们非常快速地分配大量对象，但高并发和高吞吐量会很快导致流失率超出JVM可以维持的程度。 另一方面，事务内存使用量(`Transactional Memory Usage`)描述了一个事务在完成之前，它保持的活动的内存量。 （例如，单个事务可能需要至少5MB内存并创建1000个临时对象）。 高并发意味着有许多活动事务，每个事务都需要一些内存。 如果它的总和（100 个并发事务就需要500 MB内存）超出了年轻代的容量，那么临时对象将被移到老年代。由于这个问题只在高负载情况下才能显现出来，所以它在开发过程中很容易被忽视。

#### 表现症状

因为JVM需要更频繁地执行在年轻代上执行GC，因此，单独的高流失率会拖慢程序。只有当大部分对象被回收，年轻代上的GC才会很便宜。在伴有很多并发事务的高负载情况下，许多对象在GC时将处于活动状态。因此，高事务内存使用量即意味着年轻代上有很多活动对象，这会导致耗时更长、更昂贵的年轻代的GC。如果事务内存使用量超过了年轻代的容量（如前所述），对象会过早地移入老年代，老年代的使用率就会增长。这会导致老年代上更频繁的GC，从而进一步降低性能。

在极端情况下，老年代上内存使用量会超出容量，最终以内存不足的错误而告终。棘手的部分是内存不足错误将中止所有正在运行的事务，后续GC将从内存中删除root cause。大多数内存工具只会每隔几秒就会查看Java内存，并且永远不会察觉到100％的利用率。所以，也就无法解释内存不足的错误。

#### 解决方法

这个问题的表现形式很清晰：频繁的小型的、昂贵的GC，最终导致老年代使用量的增加。解决方法也是清晰的，但是也包含了大量的测试和后续的代码修改。

- 进行全面的内存分配分析来降低流失率
- 在满载的情况下截取几个heap dump。分析一个独立的事务保存了多少活动的内存，并尝试降低活动内存量。如果产品系统中你期望的并发越多，一个独立的事务使用的内存就应该越少。
- 确保通过年轻代的练习和广泛的负载测试来跟进优化

优化流失率问题是个困难的任务，但是带来的性能和可扩展性的提升也是可观的。

### Equals和Hashcode的错误实现

hashcode方法、equals方法和内存问题的关系不是那么明显。但是我们只需要想想hashmaps就能明白它们的关系。一个对象的hashcoe用来在hashmap中插入和查找对象。但是，hashcode不是唯一的，这就是为什么它只选择一个可能包含多个对象的容器。这是由于这个原因，equals方法用来确认我们能找到正确的对象。如果hashcode方法是错误的（这可能导致其他方面相同的对象产生不同的结果），在hashmap中也就永远不会找到需要的对象。带来的后果就是程序反复在hashmap中插入对象。

尽管大多数工具都可以轻松识别不断增长的集合，但在heap dump中根本原因却并不明显。近年来我多次遇到这种情况，在一个极端的案例中，客户的JVM占用了40 GB的内存。 JVM需要每天重启一次，以避免内存不足错误。在修复了问题后，现在应用程序运行稳定在800 MB！

在这种情况下，即使能够提供对象的完整信息，heap dump的用处也不大。人们只需要分析大量的对象来识别问题。最好的方法是主动并使用自动化单元测试。一些免费框架（例如EqualsVerifier）可以确保equals和hashcode方法符合预期。





# Tip

## 利用`time`获取命令/脚本执行时间

要想获取某个命令/脚本的执行时间，直接使用`time`命令：

```bash
>time sleep 2

real	0m2.004s
user	0m0.000s
sys	0m0.001s
```

但是很多场合，我们只需要real时间。那怎么办呢？按照`man time`给出的建议，通过增加`-f`来修改默认的输出格式：

```shell
>time -f "%e" sleep 2
-bash: -f: command not found

real	0m0.009s
user	0m0.000s
sys	0m0.001s
```

奇怪的是，`time`竟然抱怨`-f: command not found`。但是如果做如下的修改，一起都很正常：

```shell
>/usr/bin/time -f "%e" sleep 2
2.00
```

很显然，问题出在了`time`和`/usr/bin/time`的差别上，两者肯定不是一回事。

这里借助[type](http://linuxcommand.org/lc3_man_pages/typeh.html)来看看系统中有哪些`time`

```shell
$ type -a time
time is a shell keyword
time is /usr/bin/time
```

由此可知，系统中有两个`time`

- Bash Shell中内置的一个关键字
- 可执行文件`/usr/bin/time`

由于shell的关键字要比可执行文件的优先级要高，所以不显式地写明路径`/usr/bin/time`或者使用`\`禁止对别名展开的话，实际运行的`time`还是shell的关键字。

最后，再问一个问题：使用shell关键字`time`，是不是就不能定制输出了呢？当然不是了，具体可以通过修改环境变量[`TIMEFORMAT`](https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html)来实现：

```shell
>TIMEFORMAT=%3R
>time sleep 2
2.002
```


# Share

## InnoDB中的Buffer Pool是如何工作的

InnoDB在内存中维护了一块叫做`buffer pool`的存储空间，用来缓存数据和索引。知道InnoDB的buffer pool如何工作，并利用它把经常访问的数据缓存到内存中，对于MySQL优化，是重要的一块。

InnoDB将buffer pool作为列表进行管理，管理方法基于`LRU`(`the leastrecently used`）算法的一种变体形式。当需要空间向pool中添加新页面时，InnoDB会删除最近最少使用的页面，并将新页面添加到列表中间。这种“中点插入策略(`midpoint insertion strategy`)”将列表视为两个子列表：

- list 头部存放的是最近经常访问的数据所在的页面，即young page或者new page。
- list尾部存放的就是最近被较少访问的数据所在的页面，即old page。

这个算法就保证了最近经常使用的页面会被保存在new sublist。不被经常访问的页面就会保存在old sublist。一旦有新的页面被加入，old sublist当中的某个page就会被删除。 

LRU算法默认的工作方式为：

- buffer pool的3/8的容量被划作old list。
- list中所谓的中点(`midpoint`)，就是old list头部和new list尾部的连接点，
- 当InnoDB将页面读入buffer pool时，它最初将该页面在中点插入，即在old list的头部。什么情况下，页面会被读入呢？执行用户指定的操作（例如SQL查询），或是InnoDB自动执行的预读操作([read-ahead](https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_read_ahead) operation)的一部分，就会导致页面被读入。
- 如果是old list的数据被访问到了，这个页信息就会变成变成young page，并被移到list的头部，也就是new sublist的头部。
- 随着DB的操作，在buffer pool中的长时间没有被访问的页面会被移向list的尾部，从而慢慢变`老`。当某些页面变`新`的时候，在new sublist和old sublit中别的页面就会变`老`。当有新的页面在中点插入时，old sublist中的页面也会变`老`。最终，某个页面如果一直未被访问，并达到了old list的尾部，那么该页面就会被删除。

https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html

http://www.cnblogs.com/shengdimaya/p/5936461.html

默认情况下，页信息会被查询语句立马查询到而被移动到new sublist，这就意味着他们会在buffer pool里面保留很长一段时间。表扫描（包括mysqldump或者没有where条件的select等操作）等操作将会刷入大量的数据进入buffer pool，同时也会将更多的buffer pool当中的信息刷出去，即使这个操作可能只会使用到一次而已。同样的如果 read-ahead后台进程读入大量数据的情况下也是会造成buffer pool大量高频的刷新数据页，但是这些操作是可控的，下面3，4会说得到。read-ahead操作简单说一下就是MySQL的一个后台预读进程，能够保证MySQL预读入数据进入buffer pool当中。